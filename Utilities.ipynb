{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b890a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71e88cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsampling\n",
    "def upsampling(df_score, df_feature, df_meta, upsample_size, counts, score):\n",
    "    upsampled_df_score = pd.DataFrame()\n",
    "    upsampled_df_feature = pd.DataFrame()\n",
    "    upsampled_df_meta = pd.DataFrame()\n",
    "\n",
    "    for i,count in enumerate(counts):\n",
    "        if count == max(counts):\n",
    "            upsampled_score_group = df_score.loc[score==i]\n",
    "            upsampled_feature_group = df_feature.loc[score==i]\n",
    "            upsampled_meta_group = df_meta.loc[score==i]    \n",
    "        else:\n",
    "            df_score_group = df_score.loc[score==i]\n",
    "            df_feature_group = df_feature.loc[score==i]\n",
    "            df_meta_group = df_meta.loc[score==i]\n",
    "            idx_list = df_feature_group.index.tolist()\n",
    "            resample_idx = resample(idx_list, replace=True, n_samples=upsample_size, random_state=27)\n",
    "            upsampled_score_group = df_score_group.loc[resample_idx]\n",
    "            upsampled_feature_group = df_feature_group.loc[resample_idx]\n",
    "            upsampled_meta_group = df_meta_group.loc[resample_idx]\n",
    "            \n",
    "        upsampled_df_score = pd.concat([upsampled_df_score, upsampled_score_group])\n",
    "        upsampled_df_feature = pd.concat([upsampled_df_feature, upsampled_feature_group])\n",
    "        upsampled_df_meta = pd.concat([upsampled_df_meta, upsampled_meta_group])\n",
    "\n",
    "    upsampled_df_score = upsampled_df_score.reset_index(drop=True)\n",
    "    upsampled_df_feature = upsampled_df_feature.reset_index(drop=True)\n",
    "    upsampled_df_meta = upsampled_df_meta.reset_index(drop=True)\n",
    "\n",
    "    return upsampled_df_score, upsampled_df_feature, upsampled_df_meta\n",
    "\n",
    "# Try other upsmapling methods:\n",
    "# from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "054004ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train/validation/test datasets\n",
    "# df: data frame containing selected features\n",
    "# score: classification label\n",
    "# subject_id: the subject id column of data frame containing classification label (score)\n",
    "def train_val_test_split(df, score, subject_id):\n",
    "    # train:validation:test = 0.5:0.25:0.25\n",
    "    sb_train = ['6_BOS', '16_BOS', '7_NYC', '14_BOS', '8_NYC', '5_BOS', '12_NYC', '6_NYC', '17_BOS',\n",
    "               '4_BOS', '11_BOS', '10_BOS', '15_BOS', '4_NYC', '11_NYC']\n",
    "    sb_val = ['8_BOS', '18_BOS', '2_NYC', '9_NYC', '3_BOS', '9_BOS']\n",
    "    sb_test = ['19_BOS', '3_NYC', '7_BOS', '5_NYC', '13_BOS', '10_NYC', '12_BOS']\n",
    "\n",
    "    # Train/Validation/Test Split\n",
    "    is_train = subject_id.isin(sb_train).tolist()\n",
    "    is_val = subject_id.isin(sb_val).tolist()\n",
    "    is_test = subject_id.isin(sb_test).tolist()\n",
    "    is_train_val = subject_id.isin(sb_train) | subject_id.isin(sb_val)\n",
    "    is_train_val = is_train_val.tolist()\n",
    "\n",
    "    X_train_valid = df.loc[is_train_val]\n",
    "    y_train_valid = score[is_train_val]\n",
    "    X_train = df.loc[is_train]\n",
    "    y_train = score[is_train]\n",
    "    X_valid = df.loc[is_val]\n",
    "    y_valid = score[is_val]\n",
    "    X_test = df.loc[is_test]\n",
    "    y_test = score[is_test]\n",
    "    \n",
    "    return X_train_valid, y_train_valid, X_train, y_train,  X_valid, y_valid, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28a1b4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing columns with zero variance in a panda dataframe using sklearn- VarianceThreshold\n",
    "def pdVarianceThreshold(df, varThreshold):\n",
    "    sel = VarianceThreshold(threshold=varThreshold)\n",
    "    new_df = sel.fit_transform(df)\n",
    "    new_filter = sel.get_support()\n",
    "    feature_names = df.columns\n",
    "    new_feature_names = feature_names[new_filter]\n",
    "    new_df = pd.DataFrame(new_df, columns=new_feature_names)\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b47f17f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns with relevant features in a panda dataframe using sklearn- Univariate Selection\n",
    "def pdSelectKBest(df, score, score_function, k_num):\n",
    "    test = SelectKBest(score_func=score_function, k=k_num)\n",
    "    new_df = test.fit_transform(df, score)\n",
    "    new_filter = test.get_support()\n",
    "    feature_names = df.columns\n",
    "    new_feature_names = feature_names[new_filter]\n",
    "    new_df = pd.DataFrame(new_df, columns=new_feature_names)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97ef4197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best classifier based on recall threshold for the positive class and F1 score\n",
    "def SelectBestClf(valid_scores, recall_1_threshold, clf_best_params):\n",
    "    is_good_recall_1 = valid_scores['Recall_1'] > recall_1_threshold\n",
    "    if sum(is_good_recall_1) == 0: # recall 1 is below the threshold \n",
    "        best_recall_1 = valid_scores['Recall_1'].max()\n",
    "        is_best = valid_scores['Recall_1'] == best_recall_1\n",
    "        if sum(is_best)>1: # duplicate max recall scores -> compare F1 score\n",
    "            max_index = valid_scores['F1_micro'].loc[is_best].idxmax()\n",
    "        else:\n",
    "            max_index = valid_scores['Recall_1'].idxmax() \n",
    "        best_F1_micro = valid_scores['F1_micro'].iloc[max_index]\n",
    "\n",
    "    else: # recall 1 is above the threshold -> select the best classifier based on F1 score\n",
    "        best_F1_micro = valid_scores['F1_micro'].loc[is_good_recall_1].max()\n",
    "        is_best = valid_scores['F1_micro'] == best_F1_micro\n",
    "        if sum(is_best)>1: # duplicate max F1 scores -> compare recall score\n",
    "            max_index = valid_scores['Recall_1'].loc[is_best].idxmax()\n",
    "        else:\n",
    "            max_index = [index for index, element in enumerate(is_best) if element] \n",
    "            max_index = max_index[0]\n",
    "        best_recall_1 = valid_scores['Recall_1'].iloc[max_index]\n",
    "\n",
    "    best_clf = valid_scores['Classifer'].iloc[max_index]\n",
    "    training_time = valid_scores['Training time'].iloc[max_index]\n",
    "    best_clf_params = clf_best_params[best_clf] \n",
    "\n",
    "    return best_F1_micro, best_recall_1, best_clf, best_clf_params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d93aea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
